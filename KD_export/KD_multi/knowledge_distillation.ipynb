{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing dataset...\n",
      "\n",
      "\n",
      "Class-to-Index Mapping:\n",
      "{'bottle': 0, 'cable': 1, 'capsule': 2, 'carpet': 3, 'grid': 4, 'hazelnut': 5, 'leather': 6, 'metal_nut': 7, 'pill': 8, 'screw': 9, 'tile': 10, 'toothbrush': 11, 'transistor': 12, 'wood': 13, 'zipper': 14}\n",
      "\n",
      "Verification of Dataset Integrity:\n",
      "+-----------+--------------------------+---------------------+\n",
      "| Dataset   | Class-to-Index Matches   | Class Names Match   |\n",
      "+===========+==========================+=====================+\n",
      "| Training  | True                     | True                |\n",
      "+-----------+--------------------------+---------------------+\n",
      "| Test      | True                     | True                |\n",
      "+-----------+--------------------------+---------------------+\n",
      "\n",
      "Dataset Statistics:\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Dataset    |   Total Samples | Class Distribution                                                                                                           |\n",
      "+============+=================+==============================================================================================================================+\n",
      "| Training   |            2903 | {5: 313, 4: 211, 3: 224, 6: 196, 9: 256, 12: 170, 1: 179, 14: 192, 0: 167, 13: 198, 10: 184, 11: 48, 7: 176, 8: 214, 2: 175} |\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Validation |             726 | {2: 44, 0: 42, 12: 43, 5: 78, 3: 56, 6: 49, 9: 64, 10: 46, 8: 53, 4: 53, 7: 44, 1: 45, 13: 49, 14: 48, 11: 12}               |\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Test       |             467 | {0: 20, 1: 58, 2: 23, 3: 28, 4: 21, 5: 40, 6: 32, 7: 22, 8: 26, 9: 41, 10: 33, 11: 12, 12: 60, 13: 19, 14: 32}               |\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from mvtec import trainloader, valloader, testloader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check current device: \n",
      "Using GPU: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "print(\"Check current device: \")\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available(): # Should return True \n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\") # Should show your GPU name\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model classes\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 56 * 56, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(8 * 56 * 56, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy runs\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, valloader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training Step\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation Step\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() \n",
    "\n",
    "        avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return train_losses, val_losses \n",
    "\n",
    "def test(model, testloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\t\n",
    "            # Collect predictions and true labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics using sklearn\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions, output_dict=True)\n",
    "\n",
    "    return cm, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracy=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "pretrained_teacher_model = \"best_model_mvtec_v1.pth\"\n",
    "use_cuda=True\n",
    "\n",
    "# Initialize the network\n",
    "torch.manual_seed(42)\n",
    "teacher_model = DeepNN(num_classes=15).to(device)\n",
    "\n",
    "# Load the pretrained model\n",
    "checkpoint = torch.load(\"best_model_mvtec_v1.pth\", map_location=device, weights_only=True)\n",
    "teacher_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_deep = test(teacher_model, testloader, device)\n",
    "test_accuracy_deep = test_deep[1][\"accuracy\"] * 100\n",
    "all_accuracy.append(test_accuracy_deep)\n",
    "print(f\"Teacher Accuracy: {test_accuracy_deep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiate the student model.\n",
      "Instantiate a copy of the student model.\n",
      "######################################################################\n",
      "To ensure we have created a copy of the student network, we inspect the norm of its first layer.\n",
      "If it matches, then we are safe to conclude that the networks are indeed the same.\n",
      "Norm of 1st layer of nn_light: 1.6299890279769897\n",
      "Norm of 1st layer of new_nn_light: 1.6299890279769897\n",
      "######################################################################\n",
      "The total number of parameters in each model\n",
      "Teacher model parameters: 12,883,295\n",
      "Student model parameters: 3,214,135\n",
      "\n",
      "######################################################################\n",
      "Cross-entropy runs with student model: \n",
      "Epoch 1/10, Loss: 0.8696271515154577\n",
      "Epoch 1/10, Validation Loss: 0.1832\n",
      "Epoch 2/10, Loss: 0.2652228517191751\n",
      "Epoch 2/10, Validation Loss: 0.1693\n",
      "Epoch 3/10, Loss: 0.19390115448898013\n",
      "Epoch 3/10, Validation Loss: 0.0950\n",
      "Epoch 4/10, Loss: 0.17692604712159424\n",
      "Epoch 4/10, Validation Loss: 0.2338\n",
      "Epoch 5/10, Loss: 0.11954715637142187\n",
      "Epoch 5/10, Validation Loss: 0.0669\n",
      "Epoch 6/10, Loss: 0.1378234244606734\n",
      "Epoch 6/10, Validation Loss: 0.0467\n",
      "Epoch 7/10, Loss: 0.07723680921001258\n",
      "Epoch 7/10, Validation Loss: 0.0227\n",
      "Epoch 8/10, Loss: 0.05189432975117138\n",
      "Epoch 8/10, Validation Loss: 0.0694\n",
      "Epoch 9/10, Loss: 0.05109507692875443\n",
      "Epoch 9/10, Validation Loss: 0.0289\n",
      "Epoch 10/10, Loss: 0.05062842082646726\n",
      "Epoch 10/10, Validation Loss: 0.0521\n",
      "Student Accuracy: 97.22%\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiate the student model.\")\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=15).to(device)\n",
    "print(\"Instantiate a copy of the student model.\")\n",
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=15).to(device)\n",
    "print(\"######################################################################\")\n",
    "\n",
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"To ensure we have created a copy of the student network, we inspect the norm of its first layer.\")\n",
    "print(\"If it matches, then we are safe to conclude that the networks are indeed the same.\")\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())\n",
    "\n",
    "print(\"######################################################################\")\n",
    "print(\"The total number of parameters in each model\")\n",
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in teacher_model.parameters()))\n",
    "print(f\"Teacher model parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"Student model parameters: {total_params_light}\")\n",
    "\n",
    "print()\n",
    "print(\"######################################################################\")\n",
    "print(\"Cross-entropy runs with student model: \")\n",
    "train_light_ce = train(nn_light, trainloader, valloader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_light_ce = test(nn_light, testloader, device)\n",
    "test_accuracy_light_ce = test_light_ce[1][\"accuracy\"] * 100\n",
    "print(f\"Student Accuracy: {test_accuracy_light_ce:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved student_model_smaller.pth\n"
     ]
    }
   ],
   "source": [
    "all_accuracy.append(test_accuracy_light_ce)\n",
    "torch.save(nn_light.state_dict(), \"student_model_smaller.pth\")\n",
    "print(\"Model saved student_model_smaller.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation run\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, trainloader, valloader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation Step\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = ce_loss(outputs, labels)\n",
    "                val_loss += loss.item() \n",
    "\n",
    "        avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return train_losses, val_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 100.00%\n",
      "Student accuracy without teacher: 97.22%\n",
      "Knowledge Distillation runs with the copy of the student model: \n",
      "Epoch 1/10, Loss: nan\n",
      "Epoch 1/10, Validation Loss: nan\n",
      "Epoch 2/10, Loss: nan\n",
      "Epoch 2/10, Validation Loss: nan\n",
      "Epoch 3/10, Loss: nan\n",
      "Epoch 3/10, Validation Loss: nan\n",
      "Epoch 4/10, Loss: nan\n",
      "Epoch 4/10, Validation Loss: nan\n",
      "Epoch 5/10, Loss: nan\n",
      "Epoch 5/10, Validation Loss: nan\n",
      "Epoch 6/10, Loss: nan\n",
      "Epoch 6/10, Validation Loss: nan\n",
      "Epoch 7/10, Loss: nan\n",
      "Epoch 7/10, Validation Loss: nan\n",
      "Epoch 8/10, Loss: nan\n",
      "Epoch 8/10, Validation Loss: nan\n",
      "Epoch 9/10, Loss: nan\n",
      "Epoch 9/10, Validation Loss: nan\n",
      "Epoch 10/10, Loss: nan\n",
      "Epoch 10/10, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Knowledge Distillation with T=0\n",
      "Student accuracy with CE + KD:\n",
      "Accuracy: 4.28%\n",
      "Precision: 0.00\n",
      "Recall: 0.04\n",
      "F1 Score: 0.00\n",
      "Model saved as student1_model_KD_T0.pth\n",
      "Epoch 1/10, Loss: nan\n",
      "Epoch 1/10, Validation Loss: nan\n",
      "Epoch 2/10, Loss: nan\n",
      "Epoch 2/10, Validation Loss: nan\n",
      "Epoch 3/10, Loss: nan\n",
      "Epoch 3/10, Validation Loss: nan\n",
      "Epoch 4/10, Loss: nan\n",
      "Epoch 4/10, Validation Loss: nan\n",
      "Epoch 5/10, Loss: nan\n",
      "Epoch 5/10, Validation Loss: nan\n",
      "Epoch 6/10, Loss: nan\n",
      "Epoch 6/10, Validation Loss: nan\n",
      "Epoch 7/10, Loss: nan\n",
      "Epoch 7/10, Validation Loss: nan\n",
      "Epoch 8/10, Loss: nan\n",
      "Epoch 8/10, Validation Loss: nan\n",
      "Epoch 9/10, Loss: nan\n",
      "Epoch 9/10, Validation Loss: nan\n",
      "Epoch 10/10, Loss: nan\n",
      "Epoch 10/10, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Knowledge Distillation with T=1\n",
      "Student accuracy with CE + KD:\n",
      "Accuracy: 4.28%\n",
      "Precision: 0.00\n",
      "Recall: 0.04\n",
      "F1 Score: 0.00\n",
      "Model saved as student1_model_KD_T1.pth\n",
      "Epoch 1/10, Loss: nan\n",
      "Epoch 1/10, Validation Loss: nan\n",
      "Epoch 2/10, Loss: nan\n",
      "Epoch 2/10, Validation Loss: nan\n",
      "Epoch 3/10, Loss: nan\n",
      "Epoch 3/10, Validation Loss: nan\n",
      "Epoch 4/10, Loss: nan\n",
      "Epoch 4/10, Validation Loss: nan\n",
      "Epoch 5/10, Loss: nan\n",
      "Epoch 5/10, Validation Loss: nan\n",
      "Epoch 6/10, Loss: nan\n",
      "Epoch 6/10, Validation Loss: nan\n",
      "Epoch 7/10, Loss: nan\n",
      "Epoch 7/10, Validation Loss: nan\n",
      "Epoch 8/10, Loss: nan\n",
      "Epoch 8/10, Validation Loss: nan\n",
      "Epoch 9/10, Loss: nan\n",
      "Epoch 9/10, Validation Loss: nan\n",
      "Epoch 10/10, Loss: nan\n",
      "Epoch 10/10, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Knowledge Distillation with T=2\n",
      "Student accuracy with CE + KD:\n",
      "Accuracy: 4.28%\n",
      "Precision: 0.00\n",
      "Recall: 0.04\n",
      "F1 Score: 0.00\n",
      "Model saved as student1_model_KD_T2.pth\n",
      "Epoch 1/10, Loss: nan\n",
      "Epoch 1/10, Validation Loss: nan\n",
      "Epoch 2/10, Loss: nan\n",
      "Epoch 2/10, Validation Loss: nan\n",
      "Epoch 3/10, Loss: nan\n",
      "Epoch 3/10, Validation Loss: nan\n",
      "Epoch 4/10, Loss: nan\n",
      "Epoch 4/10, Validation Loss: nan\n",
      "Epoch 5/10, Loss: nan\n",
      "Epoch 5/10, Validation Loss: nan\n",
      "Epoch 6/10, Loss: nan\n",
      "Epoch 6/10, Validation Loss: nan\n",
      "Epoch 7/10, Loss: nan\n",
      "Epoch 7/10, Validation Loss: nan\n",
      "Epoch 8/10, Loss: nan\n",
      "Epoch 8/10, Validation Loss: nan\n",
      "Epoch 9/10, Loss: nan\n",
      "Epoch 9/10, Validation Loss: nan\n",
      "Epoch 10/10, Loss: nan\n",
      "Epoch 10/10, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Knowledge Distillation with T=3\n",
      "Student accuracy with CE + KD:\n",
      "Accuracy: 4.28%\n",
      "Precision: 0.00\n",
      "Recall: 0.04\n",
      "F1 Score: 0.00\n",
      "Model saved as student1_model_KD_T3.pth\n",
      "Epoch 1/10, Loss: nan\n",
      "Epoch 1/10, Validation Loss: nan\n",
      "Epoch 2/10, Loss: nan\n",
      "Epoch 2/10, Validation Loss: nan\n",
      "Epoch 3/10, Loss: nan\n",
      "Epoch 3/10, Validation Loss: nan\n",
      "Epoch 4/10, Loss: nan\n",
      "Epoch 4/10, Validation Loss: nan\n",
      "Epoch 5/10, Loss: nan\n",
      "Epoch 5/10, Validation Loss: nan\n",
      "Epoch 6/10, Loss: nan\n",
      "Epoch 6/10, Validation Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m train_kd\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m T \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     train_light_ce_and_kd \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_knowledge_distillation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_nn_light\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_target_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mce_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     train_kd\u001b[38;5;241m.\u001b[39mappend(train_light_ce_and_kd)\n\u001b[0;32m     11\u001b[0m     test_light_ce_and_kd \u001b[38;5;241m=\u001b[39m test(new_nn_light, testloader, device)\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m, in \u001b[0;36mtrain_knowledge_distillation\u001b[1;34m(teacher, student, trainloader, valloader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     11\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[0;32m     13\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\KD_export\\KD_multi\\mvtec.py:148\u001b[0m, in \u001b[0;36mTransformedDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m--> 148\u001b[0m     sample, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m    150\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\torchvision\\datasets\\folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32md:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(\"Knowledge Distillation runs with the copy of the student model: \")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_kd=[]\n",
    "\n",
    "for T in range(0, 6):\n",
    "    train_light_ce_and_kd = train_knowledge_distillation(teacher=teacher_model, student=new_nn_light, trainloader=trainloader, valloader=valloader, epochs=10, learning_rate=0.001, T=T, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "    train_kd.append(train_light_ce_and_kd)\n",
    "    test_light_ce_and_kd = test(new_nn_light, testloader, device)\n",
    "    test_accuracy_light_ce_and_kd = test_light_ce_and_kd[1][\"accuracy\"] * 100\n",
    "    all_accuracy.append(test_accuracy_light_ce_and_kd)\n",
    "    precision_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"precision\"]\n",
    "    recall_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"recall\"]\n",
    "    f1_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    # Compare the student test accuracy with and without the teacher, after distillation\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(f\"Knowledge Distillation with T={T}\")\n",
    "    print(f\"Student accuracy with CE + KD:\")\n",
    "    print(f\"Accuracy: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
    "    # Print other value metrics:\n",
    "    print(f\"Precision: {precision_light_ce_and_kd:.2f}\")\n",
    "    print(f\"Recall: {recall_light_ce_and_kd:.2f}\")\n",
    "    print(f\"F1 Score: {f1_light_ce_and_kd:.2f}\")\n",
    "\n",
    "    torch.save(new_nn_light.state_dict(), f\"student_model_KD_T{T}.pth\")\n",
    "\n",
    "    print(f\"Model saved as student1_model_KD_T{T}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(0, 6), all_accuracy)\n",
    "plt.title('Accuracy of Student model with Knowledge Distillation in each temperature')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
