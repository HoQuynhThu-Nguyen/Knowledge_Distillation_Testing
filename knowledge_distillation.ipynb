{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing dataset...\n",
      "\n",
      "+------------+-------------------------+------------------------+\n",
      "| Tool       |   Training Images Moved |   Testing Images Moved |\n",
      "+============+=========================+========================+\n",
      "| bottle     |                     209 |                     20 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| cable      |                     224 |                     58 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| capsule    |                     219 |                     23 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| carpet     |                     280 |                     28 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| grid       |                     264 |                     21 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| hazelnut   |                     391 |                     40 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| leather    |                     245 |                     32 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| metal_nut  |                     220 |                     22 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| pill       |                     267 |                     26 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| screw      |                     320 |                     41 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| tile       |                     230 |                     33 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| toothbrush |                      60 |                     12 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| transistor |                     213 |                     60 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| wood       |                     247 |                     19 |\n",
      "+------------+-------------------------+------------------------+\n",
      "| zipper     |                     240 |                     32 |\n",
      "+------------+-------------------------+------------------------+\n",
      "\n",
      "Class-to-Index Mapping:\n",
      "{'bottle': 0, 'cable': 1, 'capsule': 2, 'carpet': 3, 'grid': 4, 'hazelnut': 5, 'leather': 6, 'metal_nut': 7, 'pill': 8, 'screw': 9, 'tile': 10, 'toothbrush': 11, 'transistor': 12, 'wood': 13, 'zipper': 14}\n",
      "\n",
      "Verification of Dataset Integrity:\n",
      "+-----------+--------------------------+---------------------+\n",
      "| Dataset   | Class-to-Index Matches   | Class Names Match   |\n",
      "+===========+==========================+=====================+\n",
      "| Training  | True                     | True                |\n",
      "+-----------+--------------------------+---------------------+\n",
      "| Test      | True                     | True                |\n",
      "+-----------+--------------------------+---------------------+\n",
      "\n",
      "Dataset Statistics:\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Dataset    |   Total Samples | Class Distribution                                                                                                           |\n",
      "+============+=================+==============================================================================================================================+\n",
      "| Training   |            2903 | {5: 313, 4: 211, 3: 224, 6: 196, 9: 256, 12: 170, 1: 179, 14: 192, 0: 167, 13: 198, 10: 184, 11: 48, 7: 176, 8: 214, 2: 175} |\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Validation |             726 | {2: 44, 0: 42, 12: 43, 5: 78, 3: 56, 6: 49, 9: 64, 10: 46, 8: 53, 4: 53, 7: 44, 1: 45, 13: 49, 14: 48, 11: 12}               |\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Test       |             467 | {0: 20, 1: 58, 2: 23, 3: 28, 4: 21, 5: 40, 6: 32, 7: 22, 8: 26, 9: 41, 10: 33, 11: 12, 12: 60, 13: 19, 14: 32}               |\n",
      "+------------+-----------------+------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from mvtec import trainloader, valloader, testloader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check current device: \n",
      "Using GPU: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "print(\"Check current device: \")\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available(): # Should return True \n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\") # Should show your GPU name\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model classes\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 56 * 56, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 56 * 56, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy runs\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, valloader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training Step\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation Step\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() \n",
    "\n",
    "        avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return train_losses, val_losses \n",
    "\n",
    "def test(model, testloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\t\n",
    "            # Collect predictions and true labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics using sklearn\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions, output_dict=True)\n",
    "\n",
    "    return cm, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiate the teacher model.\n",
      "Cross-entropy runs with teacher model: \n",
      "Epoch 1/10, Loss: 0.7699592949024269\n",
      "Epoch 1/10, Validation Loss: 0.1657\n",
      "Epoch 2/10, Loss: 0.2302488795798886\n",
      "Epoch 2/10, Validation Loss: 0.1574\n",
      "Epoch 3/10, Loss: 0.1352987354774806\n",
      "Epoch 3/10, Validation Loss: 0.3259\n",
      "Epoch 4/10, Loss: 0.0668280154329775\n",
      "Epoch 4/10, Validation Loss: 0.0049\n",
      "Epoch 5/10, Loss: 0.09205673861440149\n",
      "Epoch 5/10, Validation Loss: 0.0028\n",
      "Epoch 6/10, Loss: 0.0445032095462071\n",
      "Epoch 6/10, Validation Loss: 0.0006\n",
      "Epoch 7/10, Loss: 0.024811760394396306\n",
      "Epoch 7/10, Validation Loss: 0.0173\n",
      "Epoch 8/10, Loss: 0.05835916720037118\n",
      "Epoch 8/10, Validation Loss: 0.0035\n",
      "Epoch 9/10, Loss: 0.09184128373949584\n",
      "Epoch 9/10, Validation Loss: 0.1235\n",
      "Epoch 10/10, Loss: 0.20570015983980988\n",
      "Epoch 10/10, Validation Loss: 0.1267\n",
      "Teacher Accuracy: 93.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Arcade Projects\\Knowledge_Distillation_Testing\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiate the teacher model.\")\n",
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=15).to(device)\n",
    "print(\"Cross-entropy runs with teacher model: \")\n",
    "train_deep = train(nn_deep, trainloader, valloader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_deep = test(nn_deep, testloader, device)\n",
    "test_accuracy_deep = test_deep[1][\"accuracy\"] * 100\n",
    "print(f\"Teacher Accuracy: {test_accuracy_deep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiate the student model.\n",
      "Instantiate a copy of the student model.\n",
      "######################################################################\n",
      "To ensure we have created a copy of the student network, we inspect the norm of its first layer.\n",
      "If it matches, then we are safe to conclude that the networks are indeed the same.\n",
      "Norm of 1st layer of nn_light: 2.327361822128296\n",
      "Norm of 1st layer of new_nn_light: 2.327361822128296\n",
      "######################################################################\n",
      "The total number of parameters in each model\n",
      "Teacher model parameters: 51,521,199\n",
      "Student model parameters: 12,851,935\n",
      "\n",
      "######################################################################\n",
      "Cross-entropy runs with student model: \n",
      "Epoch 1/10, Loss: 0.9815446384005494\n",
      "Epoch 1/10, Validation Loss: 0.2318\n",
      "Epoch 2/10, Loss: 0.22461726772343063\n",
      "Epoch 2/10, Validation Loss: 0.0766\n",
      "Epoch 3/10, Loss: 0.14815787270992667\n",
      "Epoch 3/10, Validation Loss: 0.0388\n",
      "Epoch 4/10, Loss: 0.12001734662412124\n",
      "Epoch 4/10, Validation Loss: 0.0842\n",
      "Epoch 5/10, Loss: 0.09004107477900745\n",
      "Epoch 5/10, Validation Loss: 0.0620\n",
      "Epoch 6/10, Loss: 0.1018496489889183\n",
      "Epoch 6/10, Validation Loss: 0.3098\n",
      "Epoch 7/10, Loss: 0.13198457321070217\n",
      "Epoch 7/10, Validation Loss: 0.3642\n",
      "Epoch 8/10, Loss: 0.07688034840752013\n",
      "Epoch 8/10, Validation Loss: 0.0210\n",
      "Epoch 9/10, Loss: 0.03468049043958372\n",
      "Epoch 9/10, Validation Loss: 0.0112\n",
      "Epoch 10/10, Loss: 0.06038397807192318\n",
      "Epoch 10/10, Validation Loss: 0.0622\n",
      "Student Accuracy: 94.65%\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiate the student model.\")\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=15).to(device)\n",
    "print(\"Instantiate a copy of the student model.\")\n",
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=15).to(device)\n",
    "print(\"######################################################################\")\n",
    "\n",
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"To ensure we have created a copy of the student network, we inspect the norm of its first layer.\")\n",
    "print(\"If it matches, then we are safe to conclude that the networks are indeed the same.\")\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())\n",
    "\n",
    "print(\"######################################################################\")\n",
    "print(\"The total number of parameters in each model\")\n",
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"Teacher model parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"Student model parameters: {total_params_light}\")\n",
    "\n",
    "print()\n",
    "print(\"######################################################################\")\n",
    "print(\"Cross-entropy runs with student model: \")\n",
    "train_light_ce = train(nn_light, trainloader, valloader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_light_ce = test(nn_light, testloader, device)\n",
    "test_accuracy_light_ce = test_light_ce[1][\"accuracy\"] * 100\n",
    "print(f\"Student Accuracy: {test_accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation run\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, trainloader, valloader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation Step\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = ce_loss(outputs, labels)\n",
    "                val_loss += loss.item() \n",
    "\n",
    "        avg_val_loss = val_loss / len(valloader)  # Average validation loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return train_losses, val_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy runs with the copy of the student model: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_knowledge_distillation() got an unexpected keyword argument 'train_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-entropy runs with the copy of the student model: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train_light_ce_and_kd \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_knowledge_distillation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn_deep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_nn_light\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_target_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mce_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m test_light_ce_and_kd \u001b[38;5;241m=\u001b[39m test(new_nn_light, testloader, device)\n\u001b[0;32m      5\u001b[0m test_accuracy_light_ce_and_kd \u001b[38;5;241m=\u001b[39m test_light_ce_and_kd[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_knowledge_distillation() got an unexpected keyword argument 'train_loader'"
     ]
    }
   ],
   "source": [
    "print(\"Cross-entropy runs with the copy of the student model: \")\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_light_ce_and_kd = train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=trainloader, val_loader=valloader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_light_ce_and_kd = test(new_nn_light, testloader, device)\n",
    "test_accuracy_light_ce_and_kd = test_light_ce_and_kd[1][\"accuracy\"] * 100\n",
    "precision_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"precision\"]\n",
    "recall_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"recall\"]\n",
    "f1_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Student accuracy with CE + KD:\")\n",
    "print(f\"Accuracy: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
    "# Print other value metrics:\n",
    "print(f\"Precision: {precision_light_ce_and_kd:.2f}\")\n",
    "print(f\"Recall: {recall_light_ce_and_kd:.2f}\")\n",
    "print(f\"F1 Score: {f1_light_ce_and_kd:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle(\"Training and Validation Loss of Student model\", fontsize=16)\n",
    "\n",
    "# Visualize the loss scores of student model - ce\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 11), train_light_ce[0], label=\"Train Loss\")\n",
    "plt.plot(range(1, 11), train_light_ce[1], label=\"Validation Loss\")\n",
    "plt.axhline(y=test_accuracy_light_ce, color='red', linestyle='--', label='Test Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('with Cross-Entropy')\n",
    "\n",
    "# Visualize the loss scores of student model - ce + kd\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 11), train_light_ce_and_kd[0], label=\"Train Loss\")\n",
    "plt.plot(range(1, 11), train_light_ce_and_kd[1], label=\"Validation Loss\")\n",
    "plt.axhline(y=test_accuracy_light_ce_and_kd, color='red', linestyle='--', label='Test Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('with Knowledge Distillation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = test_light_ce_and_kd[0]\n",
    "# Visualize the confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(24, 24)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
    "plt.title(\"Confusion Matrix of Student model with Knowledge Distillation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
