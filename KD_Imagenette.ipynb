{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Distillation Tutorial\n",
    "===============================\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enviroment Setup\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  2 15:53:59 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| 30%   41C    P2             N/A /  115W |    3247MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       496    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A      1392    C+G   ...nzyj5cx40ttqa\\iCloud\\iCloudHome.exe      N/A      |\n",
      "|    0   N/A  N/A      2248    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A      4040    C+G   ...on\\131.0.2903.63\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      5880    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      7784    C+G   ..._m7qx9dzpwqaze\\app\\Twinkle Tray.exe      N/A      |\n",
      "|    0   N/A  N/A     10276    C+G   C:\\Windows\\System32\\mmgaserver.exe          N/A      |\n",
      "|    0   N/A  N/A     10560    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11996    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     12584    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12860    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13440    C+G   ...mpt_builder\\LogiAiPromptBuilder.exe      N/A      |\n",
      "|    0   N/A  N/A     14424    C+G   ...werToys\\PowerToys.PowerLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     14484    C+G   ...on\\131.0.2903.63\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     15092    C+G   ...UI3Apps\\PowerToys.AdvancedPaste.exe      N/A      |\n",
      "|    0   N/A  N/A     18216    C+G   ...ys\\WinUI3Apps\\PowerToys.Peek.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     18248    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     19204    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     19308    C+G   ...t Office\\root\\Office16\\POWERPNT.EXE      N/A      |\n",
      "|    0   N/A  N/A     20252    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     21984    C+G   ...werToys\\PowerToys.ColorPickerUI.exe      N/A      |\n",
      "|    0   N/A  N/A     22112    C+G   ...\\PowerToys\\PowerToys.FancyZones.exe      N/A      |\n",
      "|    0   N/A  N/A     22340    C+G   ...al\\Discord\\app-1.0.9172\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A     22736    C+G   ...on\\131.0.2903.70\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     23304    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     23320    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     24308    C+G   ...on\\131.0.2903.63\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     24736    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     27680    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A     27756    C+G   ...s\\System32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A     29220    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     30708    C+G   ...804_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     30940    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     36252    C+G   ...7.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     37820      C   ...rograms\\Python\\Python310\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook is located in: d:\\Arcade_Pj\\AA\\Knowledge_Distillation_Testing\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Print the current working directory\n",
    "print(f\"The notebook is located in: {os.getcwd()}\")\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available(): # Should return True \n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\") # Should show your GPU name\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagenette (10 classes)\n",
    "------------\n",
    "split **13394** samples as **7575 (train)**, **1894 (validation)**, and **3925 (test)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz to ./data\\imagenette2.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.56G/1.56G [01:48<00:00, 14.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\imagenette2.tgz to ./data\n"
     ]
    }
   ],
   "source": [
    "# Transformations for data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Loading the Imagenette dataset: (train set will later be split into train and test)\n",
    "full_trainset = datasets.Imagenette(root='./data', split=\"train\", download=True, transform=transform)\n",
    "testset = datasets.Imagenette(root='./data', split=\"val\", download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 7575\n",
      "Number of validation samples: 1894\n",
      "Number of testing samples: 3925\n"
     ]
    }
   ],
   "source": [
    "# Split trainset into train and validation datasets (80% train, 20% val)\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders for train, validation, and test datasets\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Check if dataset loads correctly\n",
    "print(f\"Number of training samples: {len(trainset)}\")\n",
    "print(f\"Number of validation samples: {len(valset)}\")\n",
    "print(f\"Number of testing samples: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model classes and utility functions\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation Step\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() \n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)  # Average validation loss\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")    \n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\t\n",
    "            # Collect predictions and true labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics using sklearn\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions, output_dict=True)\n",
    "\n",
    "    return cm, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy runs\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_deep = test(nn_deep, test_loader, device)\n",
    "accuracy_deep = test_deep[1][\"accuracy\"] * 100\n",
    "print(f\"Teacher Accuracy: {accuracy_deep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)\n",
    "\n",
    "# Re-instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer of nn_light: 2.327361822128296\n",
      "Norm of 1st layer of new_nn_light: 2.327361822128296\n"
     ]
    }
   ],
   "source": [
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "# Print the norm of the first layer of the new lightweight model\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNN parameters: 1,233,156\n",
      "LightNN parameters: 290,868\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of parameters in each model:\n",
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 3.6878931495666505\n",
      "Epoch 1/10, Validation Loss: 3.2518\n",
      "Epoch 2/10, Training Loss: 3.0673366777420044\n",
      "Epoch 2/10, Validation Loss: 2.9660\n",
      "Epoch 3/10, Training Loss: 2.799206377696991\n",
      "Epoch 3/10, Validation Loss: 2.8494\n",
      "Epoch 4/10, Training Loss: 2.6115995453834535\n",
      "Epoch 4/10, Validation Loss: 2.7489\n",
      "Epoch 5/10, Training Loss: 2.4673521230697633\n",
      "Epoch 5/10, Validation Loss: 2.7625\n",
      "Epoch 6/10, Training Loss: 2.344096250152588\n",
      "Epoch 6/10, Validation Loss: 2.7536\n",
      "Epoch 7/10, Training Loss: 2.2454467422485354\n",
      "Epoch 7/10, Validation Loss: 2.7409\n",
      "Epoch 8/10, Training Loss: 2.1480978763580323\n",
      "Epoch 8/10, Validation Loss: 2.7685\n",
      "Epoch 9/10, Training Loss: 2.0601346199035646\n",
      "Epoch 9/10, Validation Loss: 2.7574\n",
      "Epoch 10/10, Training Loss: 1.9845949500083924\n",
      "Epoch 10/10, Validation Loss: 2.8112\n",
      "Teacher Accuracy: 35.37%\n"
     ]
    }
   ],
   "source": [
    "train(nn_light, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_light_ce = test(nn_light, test_loader, device)\n",
    "accuracy_light_ce = test_light_ce[1][\"accuracy\"] * 100\n",
    "print(f\"Teacher Accuracy: {accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation run\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 5.103708025741577\n",
      "Epoch 1/10, Validation Loss: 3.3174\n",
      "Epoch 2/10, Training Loss: 4.178694239807129\n",
      "Epoch 2/10, Validation Loss: 3.0457\n",
      "Epoch 3/10, Training Loss: 3.789828225517273\n",
      "Epoch 3/10, Validation Loss: 2.9247\n",
      "Epoch 4/10, Training Loss: 3.5352023859024047\n",
      "Epoch 4/10, Validation Loss: 2.8418\n",
      "Epoch 5/10, Training Loss: 3.3280539739608765\n",
      "Epoch 5/10, Validation Loss: 2.8029\n",
      "Epoch 6/10, Training Loss: 3.16405025100708\n",
      "Epoch 6/10, Validation Loss: 2.7923\n",
      "Epoch 7/10, Training Loss: 3.0289479677200317\n",
      "Epoch 7/10, Validation Loss: 2.8245\n",
      "Epoch 8/10, Training Loss: 2.9066384692192075\n",
      "Epoch 8/10, Validation Loss: 2.8620\n",
      "Epoch 9/10, Training Loss: 2.7955906955718994\n",
      "Epoch 9/10, Validation Loss: 2.9141\n",
      "Epoch 10/10, Training Loss: 2.70514990940094\n",
      "Epoch 10/10, Validation Loss: 2.8434\n",
      "-----------------------------------------\n",
      "Teacher accuracy: 32.23%\n",
      "Student accuracy without teacher: 35.37%\n",
      "-----------------------------------------\n",
      "Student accuracy with CE + KD:\n",
      "Accuracy: 35.36%\n",
      "Precision: 0.35\n",
      "Recall: 0.35\n",
      "F1 Score: 0.34\n"
     ]
    }
   ],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, val_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation Step\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = ce_loss(outputs, labels)\n",
    "                val_loss += loss.item() \n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)  # Average validation loss\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")   \n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, val_loader=val_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "accuracy_light_ce_and_kd = test_light_ce_and_kd[1][\"accuracy\"] * 100\n",
    "precision_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"precision\"]\n",
    "recall_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"recall\"]\n",
    "f1_light_ce_and_kd = test_light_ce_and_kd[1][\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Teacher accuracy: {accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {accuracy_light_ce:.2f}%\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Student accuracy with CE + KD:\")\n",
    "print(f\"Accuracy: {accuracy_light_ce_and_kd:.2f}%\")\n",
    "# Print other value metrics:\n",
    "print(f\"Precision: {precision_light_ce_and_kd:.2f}\")\n",
    "print(f\"Recall: {recall_light_ce_and_kd:.2f}\")\n",
    "print(f\"F1 Score: {f1_light_ce_and_kd:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = test_light_ce_and_kd[0]\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
